{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Structure from Scientific Abstracts\n",
    "### using a LSTM neural network\n",
    "\n",
    "\n",
    "_Paul Willot_\n",
    "\n",
    "\n",
    "This project was made for the [ICADL 2015](http://icadl2015.org/) conference.  \n",
    "In this notebook we will go through all steps required to build a [LSTM](https://en.wikipedia.org/wiki/Long_short_term_memory \"Long Short Term Memory\") neural network to classify sentences inside a scientific paper abstract.\n",
    "\n",
    "**Summary:**\n",
    "* [Extract dataset](#extract)\n",
    "* [Pre-process](#pre-process)\n",
    "* [Label analysis](#label analysis)\n",
    "* [Choosing labels](#choosing label)\n",
    "* [Create train and test set](#create train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Willot \n",
      "\n",
      "CPython 2.7.10\n",
      "IPython 4.0.0\n",
      "\n",
      "numpy 1.8.0rc1\n",
      "scipy 0.13.0b1\n",
      "spacy 0.89\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)\n",
      "system     : Darwin\n",
      "release    : 14.5.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "#%install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark.py\n",
    "%load_ext watermark\n",
    "# for reproducibility\n",
    "%watermark -a 'Paul Willot' -mvp numpy,scipy,spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "First, let's **gather some data**.  We use the [PubMed](http://www.ncbi.nlm.nih.gov/pubmed) database of medical paper.\n",
    "\n",
    "Specificaly, we will focus on [structured abstracts](http://www.ncbi.nlm.nih.gov/pubmed/?term=hasstructuredabstract). There is approximately 3 million avalaible, and we will focus on a reduced portion of this (500.000) but feel free to use a bigger corpus.\n",
    "\n",
    "The easiest way to try this is to use the `toy_corpus.txt` and `tokenizer.pickle` included in the [project repo](https://github.com/m3at/Labelizer).\n",
    "\n",
    "To work on real dataset, for convenience I prepared the following files. Use the one appropriate for your needs, for example you can download the training and testing datas and jump to the [next notebook](https://github.com/m3at/Labelizer/blob/master/Labelizer_part2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the **full corpus** (~500.000 structured abstracts, 500 MB compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/lhqe3bls0mkbq57/pubmed_result_548899.txt.zip -P ./data/\n",
    "!unzip -o ./data/pubmed_result_548899.txt.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a **toy corpus** (224 structured abstracts, 200 KB compressed)\n",
    "\n",
    "*__Note:__ this file is already included in the project GitHub [repository](https://github.com/m3at/Labelizer).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/ujo1l8duu31js34/toy_corpus.txt.zip -P ./data/\n",
    "#!unzip -o ./TMP/toy_corpus.txt.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a **lemmatized corpus** (preprocessed, 350 MB compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/lmv88n1vpmp6c19/corpus_lemmatized.pickle.zip -P ./data/\n",
    "!unzip -o ./data/corpus_lemmatized.pickle.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **training and testing datas** for the LSTM (preprocessed, vectorized and splitted, 100 MB compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/0o7i0ejv4aqf6gs/training_4_BacObjMetCon.pickle.zip -P ./data/\n",
    "!unzip -o ./data/training_4_BacObjMetCon.pickle.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "# import local libraries\n",
    "import tools\n",
    "import prepare\n",
    "import lemmatize\n",
    "import analyze\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extract'></a>\n",
    "## Extract and parse the dataset\n",
    "\n",
    "Separate each documents, isolate the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exctracting from 'toy_corpus'...\n",
      "224 documents exctracted - 1.9KB  [286.4KB/s]\n",
      "Done. [0.01s]\n"
     ]
    }
   ],
   "source": [
    "data = prepare.extract_txt('data/toy_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data currently look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. EJNMMI Res. 2014 Dec;4(1):75. doi: 10.1186/s13550-014-0075-x. Epub 2014 Dec 14.\n",
      "\n",
      "Labeling galectin-3 for the assessment of myocardial infarction in rats.\n",
      "\n",
      "Arias T(1), Petrov A, Chen J, de Haas H, Pérez-Medina C, Strijkers GJ, Hajjar RJ,\n",
      "Fayad ZA, Fuster V, Narula J.\n",
      "\n",
      "Author information: \n",
      "(1)Zena and Michael A. Wiener Cardiovascular Institute, Icahn School of Medicine \n",
      "at Mount Sinai, One Gustave L. Levy Place, Box 1030, New York, NY, 10029, USA,\n",
      "tvarias@cnic.es.\n",
      "\n",
      "BACKGROUND: Galectin-3 is a ß-galactoside-binding lectin expressed in most of\n",
      "tissues in normal conditions and overexpressed in myocardium from early stages of\n",
      "heart failure (HF). It is an established biomarker associated with extracellular \n",
      "matrix (ECM) turnover during myocardial remodeling. The aim of this study is to\n",
      "test t\n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "print(\"%s\\n[...]\"%data[0][:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 4 core...\n",
      "1.4KB/s on each of the [4] core\n",
      "Done. [0.35s]\n"
     ]
    }
   ],
   "source": [
    "abstracts = prepare.get_abstracts(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning, dumping the abstracts with incorrect number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_err(datas,errs):\n",
    "    err=sorted([item for subitem in errs for item in subitem],reverse=True)\n",
    "    for e in err:\n",
    "        for d in datas:\n",
    "            del d[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_err([abstracts],prepare.get_errors(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 219 documents.\n"
     ]
    }
   ],
   "source": [
    "print(\"Working on %d documents.\"%len(abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre-process'></a>\n",
    "# Pre-process\n",
    "**Replacing numbers** with ##NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering numbers...\n",
      "Done. [0.04s]\n"
     ]
    }
   ],
   "source": [
    "abstracts = prepare.filter_numbers(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **correct sentence splitting**, we train a tokenizer using NLTK Punkt Sentence Tokenizer. This tokenizer use an unsupervised algorithm to learn how to split sentences on a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence tokenizer...\n",
      "Done. [0.29s]\n",
      "Working on 4 core...\n",
      "2.0KB/s on each of the [4] core\n",
      "Done. [0.26s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = prepare.create_sentence_tokenizer(abstracts)\n",
    "# For a more general parser, use the one provided in NLTK:\n",
    "#import nltk.data\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "abstracts_labeled = prepare.ex_all_labels(abstracts,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data look now like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'BACKGROUND',\n",
       " [u'Galectin-##NB is a \\xdf-galactoside-binding lectin expressed in most of tissues in normal conditions and overexpressed in myocardium from early stages of heart failure (HF).',\n",
       "  u'It is an established biomarker associated with extracellular  matrix (ECM) turnover during myocardial remodeling.',\n",
       "  u'The aim of this study is to test the ability of (##NB)I-galectin-##NB (IG##NB) to assess cardiac remodeling in a model of myocardial infarction (MI) using imaging techniques. ']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_labeled[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "It may be a long process on huge dataset, but using spacy make it currently 50 times faster than a slimple use of the NLTK tools.  \n",
    "It get a huge speedup with paralellisation (tryed on 80 cores). Specify nb_core=X if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 4 core...\n",
      "Splitting datas... Done. [0.00s]\n",
      "Lemmatizing...\n",
      "Done. [0min 7s]\n"
     ]
    }
   ],
   "source": [
    "lemmatized = lemmatize.lemm(abstracts_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'BACKGROUND',\n",
       "  [u'galectin-##nb be a \\xdf-galactoside bind lectin express in most of tissue in normal condition and overexpressed in myocardium from early stage of heart failure hf',\n",
       "   u'it be an establish biomarker associate with extracellular matrix ecm turnover during myocardial remodeling',\n",
       "   u'the aim of this study be to test the ability of nb)i galectin-##nb ig##nb to assess cardiac remodeling in a model of myocardial infarction mi use imaging technique']],\n",
       " [u'METHODS',\n",
       "  [u'recombinant galectin-##nb be label with iodine-##nb and in vitro bind assay be conduct to test nb)i galectin-##nb ability to bind to ecm target',\n",
       "   u'for in vivo study a rat model of induce mi be use',\n",
       "   u'animal be subject to magnetic resonance and micro spetc/micro-ct image two nb w mi or four nb w mi week after mi.',\n",
       "   u'sham rat be use as control',\n",
       "   u'pharmacokinetic biodistribution and histological study be also perform after intravenous administration of ig##nb.']],\n",
       " [u'RESULTS',\n",
       "  [u'in vitro study reveal that ig##nb show higher bind affinity measure as count per minute cpm p < nb to laminin nb \\xb1 nb cpm fibronectin nb \\xb1 nb cpm and collagen type -pron- nb \\xb1 nb cpm compare to bovine serum albumin bsa nb \\xb1 nb cpm',\n",
       "   u'myocardial quantitative ig##nb uptake %id/g be high p < nb in the infarct of nb w mi rat nb \\xb1 nb compare to control nb \\xb1 nb',\n",
       "   u'ig##nb infarct uptake correlate with the extent of scar r s = nb p = nb',\n",
       "   u'total collagen deposition in the infarct percentage area be high p < nb at nb w mi nb \\xb1 nb and nb w mi nb \\xb1 nb compare to control nb \\xb1 nb',\n",
       "   u'however thick collagen content in the infarct square micrometer stain be high at nb w mi nb \\xb1 nb \\u03bcm(##nb compare to control nb \\xb1 nb \\u03bcm(##nb p < nb and nb w mi nb \\xb1 nb \\u03bcm(##nb p < nb']],\n",
       " [u'CONCLUSIONS',\n",
       "  [u'this study show although preliminary enough data to consider ig##nb as a potential contrast agent for imaging of myocardial interstitial change in rat after mi.',\n",
       "   u'label strategy need to be seek to improve in vivo ig##nb imaging and if prove galectin-##nb may be use as an imaging tool for the assessment and treatment of mi patient']]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping...\n",
      "Done. [0.05s]\n"
     ]
    }
   ],
   "source": [
    "tools.dump_pickle(lemmatized,\"data/fast_lemmatized.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To directly load a lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'data/corpus_lemmatized.pickle'...\n",
      "'data/corpus_lemmatized.pickle' not found, trying 'data/corpus_lemmatized.pickle.pickle'\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/corpus_lemmatized.pickle.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e821850b10ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/corpus_lemmatized.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/meat/Documents/NII/labelizer/tools.pyc\u001b[0m in \u001b[0;36mload_pickle\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'%s' not found, trying '%s.pickle'\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mreturnObject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/corpus_lemmatized.pickle.pickle'"
     ]
    }
   ],
   "source": [
    "lemmatized = tools.load_pickle(\"data/corpus_lemmatized.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='label analysis'></a>\n",
    "# Label analysis\n",
    "*Does not affect the corpus*, we simply do this get some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying corpus...Done. [0.02s]\n",
      "Creating dictionary of labels...\n",
      "Done. [0.00s]\n"
     ]
    }
   ],
   "source": [
    "dic = analyze.create_dic_simple(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels : 58\n",
      "195______RESULTS\n",
      "151______METHODS\n",
      "146______BACKGROUND\n",
      "117______CONCLUSIONS\n",
      "91_______CONCLUSION\n",
      "26_______INTRODUCTION\n",
      "22_______OBJECTIVE\n",
      "16_______MATERIALS AND METHODS\n",
      "10_______OBJECTIVES\n",
      "10_______PURPOSE\n",
      "...\n",
      "(48 other labels with less than 10 occurences)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of labels :\",len(dic.keys()))\n",
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_keyword=['AIM','BACKGROUND','INTRODUCTION','METHOD','RESULT','CONCLUSION','OBJECTIVE','DESIGN','FINDING','OUTCOME','PURPOSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regrouped: 31\n"
     ]
    }
   ],
   "source": [
    "analyze.regroup_keys(dic,primary_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212______CONCLUSION\n",
      "200______RESULT\n",
      "192______METHOD\n",
      "149______BACKGROUND\n",
      "33_______OBJECTIVE\n",
      "26_______INTRODUCTION\n",
      "10_______PURPOSE\n",
      "...\n",
      "(22 other labels with less than 10 occurences)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys_to_replace = [['INTRODUCTION','CONTEXT','PURPOSE'],\n",
    "                  ['AIM','SETTING'],\n",
    "                  ['FINDING','OUTCOME','DISCUSSION']]\n",
    "\n",
    "replace_with =    ['BACKGROUND',\n",
    "                  'METHOD',\n",
    "                  'CONCLUSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regplaced: 8\n"
     ]
    }
   ],
   "source": [
    "analyze.replace_keys(dic,keys_to_replace,replace_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221______CONCLUSION\n",
      "203______METHOD\n",
      "200______RESULT\n",
      "186______BACKGROUND\n",
      "33_______OBJECTIVE\n",
      "...\n",
      "(16 other labels with less than 10 occurences)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='choosing labels'></a>\n",
    "# Choosing labels\n",
    "_Does affect the corpus_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restrict our data to work only on abstracts having labels maching a **specific pattern**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    ['BACKGROUND','BACKGROUNDS'],\n",
    "    ['METHOD','METHODS'],\n",
    "    ['RESULT','RESULTS'],\n",
    "    ['CONCLUSION','CONCLUSIONS'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting abstracts...\n",
      "91/219 match the pattern (41%)\n",
      "Done. [0.00s]\n"
     ]
    }
   ],
   "source": [
    "sub_perfect = analyze.get_exactly(lemmatized,pattern=pattern,no_truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting abstracts...\n",
      "98/219 match the pattern (44%)\n",
      "Done. [0.00s]\n"
     ]
    }
   ],
   "source": [
    "sub_perfect = analyze.get_exactly(lemmatized,pattern=pattern,no_truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 abstracts labeled and ready for the next part\n"
     ]
    }
   ],
   "source": [
    "print(\"%d abstracts labeled and ready for the next part\"%len(sub_perfect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Or we can keep a more **noisy dataset** and reduce it to a set of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying corpus...Done. [0.01s]\n",
      "Creating dictionary of labels...\n",
      "Done. [0.01s]\n"
     ]
    }
   ],
   "source": [
    "dic = preprocess.create_dic(lemmatized,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regrouped: 31\n"
     ]
    }
   ],
   "source": [
    "# We can re-use the variables defined in the analysis section\n",
    "#primary_keyword=['AIM','BACKGROUND','METHOD','RESULT','CONCLUSION','OBJECTIVE','DESIGN','FINDINGS','OUTCOME','PURPOSE']\n",
    "analyze.regroup_keys(dic,primary_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regplaced: 8\n"
     ]
    }
   ],
   "source": [
    "#keys_to_replace = [['INTRODUCTION','BACKGROUND','AIM','PURPOSE','CONTEXT'],\n",
    "#                  ['CONCLUSION']]\n",
    "\n",
    "#replace_with =    ['OBJECTIVE',\n",
    "#                  'RESULT']\n",
    "\n",
    "analyze.replace_keys(dic,keys_to_replace,replace_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can restrict our analysis to the main labels\n",
    "dic = {key:dic[key] for key in ['BACKGROUND','RESULT','METHOD','CONCLUSION']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221______CONCLUSION\n",
      "203______METHOD\n",
      "200______RESULT\n",
      "186______BACKGROUND\n"
     ]
    }
   ],
   "source": [
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences per label : ['CONCLUSION 446', 'RESULT 946', 'BACKGROUND 481', 'METHOD 640']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences per label :\",[\"%s %d\"%(s,len(dic[s][1])) for s in dic.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create train'></a>\n",
    "# Creating train and test data\n",
    "\n",
    "Let's format the datas for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reorder the labels for better readability_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONCLUSION', 'RESULT', 'BACKGROUND', 'METHOD']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_names = ['BACKGROUND', 'METHOD', 'RESULT','CONCLUSION']\n",
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train/test split\n",
    "split = 0.8\n",
    "\n",
    "# truncate the number of abstracts to consider for each label,\n",
    "# -1 to set to the maximum while keeping the number of sentences per labels equal\n",
    "raw_x_train, raw_y_train, raw_x_test, raw_y_test = preprocess.split_data(dic,classes_names,\n",
    "                                                              split_train_test=split,\n",
    "                                                              truncate=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the training set...Done. [0.07s]\n",
      "Getting features...Done. [0.01s]\n",
      "Creating order...Done. [0.05s]\n",
      "Done. [0.13s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, feature_names, max_features, vectorizer = preprocess.vectorize_data(raw_x_train,\n",
    "                                                                                                      raw_y_train,\n",
    "                                                                                                      raw_x_test,\n",
    "                                                                                                      raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 4506\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features : %d\"%(max_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save all this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping...\n",
      "Done. [0.30s]\n"
     ]
    }
   ],
   "source": [
    "tools.dump_pickle([X_train, y_train, X_test, y_test, feature_names, max_features, classes_names, vectorizer],\n",
    "                  \"data/unpadded_4_BacObjMetCon.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and jump to the second notebook to train the LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
