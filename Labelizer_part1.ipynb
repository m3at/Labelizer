{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extracting Structure from Scientific Abstracts\n",
    "###using a LSTM neural network\n",
    "\n",
    "\n",
    "_Paul Willot_\n",
    "\n",
    "\n",
    "In this notebook we will go through all steps required to build a [LSTM](https://en.wikipedia.org/wiki/Long_short_term_memory \"Long Short Term Memory\") neural network to classify sentences inside a scientific paper abstract.\n",
    "\n",
    "**Summary:**\n",
    "* [Extract dataset](#extract)\n",
    "* [Pre-process](#pre-process)\n",
    "* [Label analysis](#label analysis)\n",
    "* [Choosing labels](#choosing label)\n",
    "* [Create train and test set](#create train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed watermark.py. To use it, type:\n",
      "  %load_ext watermark\n",
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "%install_ext https://raw.githubusercontent.com/rasbt/watermark/master/watermark.py\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Willot \n",
      "\n",
      "CPython 2.7.6\n",
      "IPython 3.2.0\n",
      "\n",
      "numpy 1.9.2\n",
      "scipy 0.16.0\n",
      "keras 0.1.2\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)\n",
      "system     : Darwin\n",
      "release    : 14.3.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%watermark -a 'Paul Willot' -mvp numpy,scipy,keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's gather some data. \n",
    "\n",
    "We use the [PubMed](http://www.ncbi.nlm.nih.gov/pubmed) database of medical paper.\n",
    "\n",
    "Specificaly, we will focus on [structured abstracts](http://www.ncbi.nlm.nih.gov/pubmed/?term=hasstructuredabstract). There is approximately 3 million avalaible, we will focus on a reduced portion of this (500.000) but feel free to use a bigger corpus.\n",
    "\n",
    "The easiest way to try this is to use the `toy_corpus.txt` and `tokenizer.pickle` included in the [project repo](https://github.com/m3at/Labelizer).\n",
    "\n",
    "To work on real dataset, for convenience I prepared the following files, jump to .. [link](#shortcut \"intra link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a **full corpus** (500.000 structured abstracts, 500 MB compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/lhqe3bls0mkbq57/pubmed_result_548899.txt.zip -P ./data/\n",
    "#!unzip -o ./data/pubmed_result_548899.txt.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a **toy corpus** (224 structured abstracts, 200 KB compressed)\n",
    "\n",
    "*__Note:__ this file is already included in my GitHub repository*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/ujo1l8duu31js34/toy_corpus.txt.zip -P ./data/\n",
    "#!unzip -o ./TMP/toy_corpus.txt.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a **lemmatized corpus** (preprocessed, 350 MB compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/lmv88n1vpmp6c19/corpus_lemmatized.pickle.zip -P ./data/\n",
    "#!unzip -o ./data/corpus_lemmatized.pickle.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **training and testing datas** for the LSTM (preprocessed, vectorized and splitted, 100 MB compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/0o7i0ejv4aqf6gs/training_4_BacObjMetCon.pickle.zip -P ./data/\n",
    "#!unzip -o ./data/training_4_BacObjMetCon.pickle.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunch of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "# import local library\n",
    "import tools\n",
    "import prepare\n",
    "import lemmatize\n",
    "import analyze\n",
    "import preprocess\n",
    "\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "#import nnlstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extract'></a>\n",
    "##Extract and parse the dataset\n",
    "\n",
    "Separate each documents, isolate the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exctracting from 'toy_corpus'...\n",
      "224 documents exctracted - 1.9KB  [395.3KB/s]\n",
      "Done. [0.00s]\n"
     ]
    }
   ],
   "source": [
    "data = prepare.extract_txt('data/toy_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. EJNMMI Res. 2014 Dec;4(1):75. doi: 10.1186/s13550-014-0075-x. Epub 2014 Dec 14.\n",
      "\n",
      "Labeling galectin-3 for the assessment of myocardial infarction in rats.\n",
      "\n",
      "Arias T(1), Petrov A, Chen J, de Haas H, Pérez-Medina C, Strijkers GJ, Hajjar RJ,\n",
      "Fayad ZA, Fuster V, Narula J.\n",
      "\n",
      "Author information: \n",
      "(1)Zena and Michael A. Wiener Cardiovascular Institute, Icahn School of Medicine \n",
      "at Mount Sinai, One Gustave L. Levy Place, Box 1030, New York, NY, 10029, USA,\n",
      "tvarias@cnic.es.\n",
      "\n",
      "BACKGROUND: Galectin-3 is a ß-galactoside-binding lectin expressed in most of\n",
      "tissues in normal conditions and overexpressed in myocardium from early stages of\n",
      "heart failure (HF). It is an established biomarker associated with extracellular \n",
      "matrix (ECM) turnover during myocardial remodeling. The aim of this study is to\n",
      "test t\n",
      "[...]\n"
     ]
    }
   ],
   "source": [
    "print(\"%s\\n[...]\"%data[0][:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 4 core...\n",
      "1.4KB/s on each of the [4] core\n",
      "Done. [0.34s]\n"
     ]
    }
   ],
   "source": [
    "abstracts = prepare.get_abstracts(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning, dumping the abstracts with incorrect number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_err(datas,errs):\n",
    "    err=sorted([item for subitem in errs for item in subitem],reverse=True)\n",
    "    for e in err:\n",
    "        for d in datas:\n",
    "            del d[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_err([abstracts],prepare.get_errors(abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 219 documents.\n"
     ]
    }
   ],
   "source": [
    "print(\"Working on %d documents.\"%len(abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre-process'></a>\n",
    "#Pre-process\n",
    "**Replacing numbers** with ##NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering numbers...\n",
      "Done. [0.04s]\n"
     ]
    }
   ],
   "source": [
    "abstracts = prepare.filter_numbers(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **correct sentence splitting**, let's train a tokenizer using NLTK Punkt Sentence Tokenizer. This tokenizer use an unsupervised algorithm to learn how to split sentences on a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence tokenizer...\n",
      "Done. [0.34s]\n",
      "Working on 4 core...\n",
      "1.5KB/s on each of the [4] core\n",
      "Done. [0.34s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = prepare.create_sentence_tokenizer(abstracts)\n",
    "# For a more general parser, use the one provided in NLTK:\n",
    "#import nltk.data\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "abstracts_labeled = prepare.ex_all_labels(abstracts,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'BACKGROUND',\n",
       " [u'Galectin-##NB is a \\xdf-galactoside-binding lectin expressed in most of tissues in normal conditions and overexpressed in myocardium from early stages of heart failure (HF).',\n",
       "  u'It is an established biomarker associated with extracellular  matrix (ECM) turnover during myocardial remodeling.',\n",
       "  u'The aim of this study is to test the ability of (##NB)I-galectin-##NB (IG##NB) to assess cardiac remodeling in a model of myocardial infarction (MI) using imaging techniques. ']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_labeled[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 4 core...\n",
      "Splitting datas... Done. [0.00s]\n",
      "Lemmatizing...\n",
      "Done. [0min 9s]\n"
     ]
    }
   ],
   "source": [
    "lemmatized = lemmatize.lemm(abstracts_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping...\n",
      "Done. [0.04s]\n"
     ]
    }
   ],
   "source": [
    "tools.dump_pickle(lemmatized,\"data/fast_lemmatized.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To directly load a lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'data/corpus_lemmatized.pickle'...\n",
      "Done. [1903.28s]\n"
     ]
    }
   ],
   "source": [
    "lemmatized = tools.load_pickle(\"data/corpus_lemmatized.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='label analysis'></a>\n",
    "#Label analysis\n",
    "*Does not affect the corpus*, simply to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying corpus...Done. [0.01s]\n",
      "Creating dictionary of labels...\n",
      "Done. [0.00s]\n"
     ]
    }
   ],
   "source": [
    "dic = analyze.create_dic_simple(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels : 58\n",
      "195______RESULTS\n",
      "151______METHODS\n",
      "146______BACKGROUND\n",
      "117______CONCLUSIONS\n",
      "91_______CONCLUSION\n",
      "26_______INTRODUCTION\n",
      "22_______OBJECTIVE\n",
      "16_______MATERIALS AND METHODS\n",
      "10_______OBJECTIVES\n",
      "10_______PURPOSE\n",
      "...\n",
      "(48 other labels with less than 10 occurences)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of labels :\",len(dic.keys()))\n",
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_keyword=['AIM','BACKGROUND','INTRODUCTION','METHOD','RESULT','CONCLUSION','OBJECTIVE','DESIGN','FINDING','OUTCOME','PURPOSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regrouped: 31\n"
     ]
    }
   ],
   "source": [
    "analyze.regroup_keys(dic,primary_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212______CONCLUSION\n",
      "200______RESULT\n",
      "192______METHOD\n",
      "149______BACKGROUND\n",
      "33_______OBJECTIVE\n",
      "26_______INTRODUCTION\n",
      "10_______PURPOSE\n",
      "...\n",
      "(22 other labels with less than 10 occurences)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys_to_replace = [['INTRODUCTION','CONTEXT','PURPOSE'],\n",
    "                  ['AIM','SETTING'],\n",
    "                  ['FINDING','OUTCOME','DISCUSSION']]\n",
    "\n",
    "replace_with =    ['BACKGROUND',\n",
    "                  'METHOD',\n",
    "                  'CONCLUSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regplaced: 8\n"
     ]
    }
   ],
   "source": [
    "analyze.replace_keys(dic,keys_to_replace,replace_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221______CONCLUSION\n",
      "203______METHOD\n",
      "200______RESULT\n",
      "186______BACKGROUND\n",
      "33_______OBJECTIVE\n",
      "...\n",
      "(16 other labels with less than 10 occurences)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='choosing labels'></a>\n",
    "#Choosing labels\n",
    "_Does affect the corpus_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restrict our data only on abstracts having labels maching a **specific pattern**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    ['BACKGROUND','BACKGROUNDS'],\n",
    "    ['METHOD','METHODS'],\n",
    "    ['RESULT','RESULTS'],\n",
    "    ['CONCLUSION','CONCLUSIONS'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting abstracts...\n",
      "91/219 match the pattern (41%)\n",
      "Done. [0.00s]\n"
     ]
    }
   ],
   "source": [
    "sub_perfect = analyze.get_exactly(lemmatized,pattern=pattern,no_truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting abstracts...\n",
      "98/219 match the pattern (44%)\n",
      "Done. [0.00s]\n"
     ]
    }
   ],
   "source": [
    "sub_perfect = analyze.get_exactly(lemmatized,pattern=pattern,no_truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 abstracts labeled and ready for the next part!\n"
     ]
    }
   ],
   "source": [
    "print(\"%d abstracts labeled and ready for the next part!\"%len(sub_perfect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can keep a more **noisy dataset** and reduce it to a set of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying corpus...Done. [0.02s]\n",
      "Creating dictionary of labels...\n",
      "Done. [0.01s]\n"
     ]
    }
   ],
   "source": [
    "dic = preprocess.create_dic(lemmatized,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regrouped: 31\n"
     ]
    }
   ],
   "source": [
    "#primary_keyword=['AIM','BACKGROUND','METHOD','RESULT','CONCLUSION','OBJECTIVE','DESIGN','FINDINGS','OUTCOME','PURPOSE']\n",
    "analyze.regroup_keys(dic,primary_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys regplaced: 8\n"
     ]
    }
   ],
   "source": [
    "#keys_to_replace = [['INTRODUCTION','BACKGROUND','AIM','PURPOSE','CONTEXT'],\n",
    "#                  ['CONCLUSION']]\n",
    "\n",
    "#replace_with =    ['OBJECTIVE',\n",
    "#                  'RESULT']\n",
    "\n",
    "analyze.replace_keys(dic,keys_to_replace,replace_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = {key:dic[key] for key in ['BACKGROUND','RESULT','METHOD','CONCLUSION']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221______CONCLUSION\n",
      "203______METHOD\n",
      "200______RESULT\n",
      "186______BACKGROUND\n"
     ]
    }
   ],
   "source": [
    "analyze.show_keys(dic,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences per label : ['OBJECTIVE 56', 'METHOD 640', 'BACKGROUND 470', 'CONCLUSION 1392']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences per label :\",[\"%s %d\"%(s,len(dic[s][1])) for s in dic.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create train'></a>\n",
    "#Creating train and test data\n",
    "\n",
    "Let's format the datas for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reorder the labels for better readability_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONCLUSION', 'RESULT', 'BACKGROUND', 'METHOD']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_names = ['BACKGROUND', 'METHOD', 'RESULT','CONCLUSION']\n",
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train/test split\n",
    "split = 0.8\n",
    "\n",
    "# truncate the number of abstracts to consider for each label,\n",
    "# -1 to set to the maximum while keeping the number of sentences per labels equal\n",
    "raw_x_train, raw_y_train, raw_x_test, raw_y_test = preprocess.split_data(dic,classes_names,\n",
    "                                                              split_train_test=split,\n",
    "                                                              truncate=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the training set...Done. [0.06s]\n",
      "Getting features...Done. [0.01s]\n",
      "Creating order...Done. [0.05s]\n",
      "Done. [0.12s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, feature_names, max_features, vectorizer = preprocess.vectorize_data(raw_x_train, raw_y_train, raw_x_test, raw_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 4532\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features : %d\"%(max_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save all this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping...\n",
      "Done. [0.38s]\n"
     ]
    }
   ],
   "source": [
    "tools.dump_pickle([X_train, y_train, X_test, y_test, feature_names, max_features, classes_names, vectorizer],\"data/unpadded_4_BacObjMetCon.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and jump to the other notebook to train the LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
